{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5a15188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Shape: (1479, 38)\n",
      "First 5 rows (z-values should now be typically between -1 and 1):\n",
      "    set_id  time_dataset  type_adhesion  type_flank_wear  type_flank_wear+adhesion   wear           z0           z1           z2           z3           z4           z5            z6           z7           z8           z9          z10          z11          z12          z13          z14          z15          z16          z17          z18          z19          z20           z21          z22           z23          z24          z25          z26           z27          z28          z29          z30           z31\n",
      "18       1             6              1                0                         0   75.0 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000\n",
      "19       1             8              1                0                         0   45.0 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.7219238094 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 -0.5031689453 0.0000000000 0.0000000000 0.0000000000  0.4750231845 0.0000000000 0.0000000000 0.0000000000  0.0000000000\n",
      "20       1             9              1                0                         0   60.0 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000 -1.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000\n",
      "21       1            11              1                0                         0   60.0 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000\n",
      "0        1            27              0                1                         0   90.0 0.0000000000 0.0000000000 0.0000000000 0.8560216173 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.4980404168 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000 -0.1385017473\n",
      "1        1            28              0                1                         0   90.0 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.9968931683 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0787655447\n",
      "2        1            35              0                1                         0   90.0 0.0000000000 0.0000000000 0.0000000000 1.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000\n",
      "22       1            39              1                0                         0   90.0 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000\n",
      "3        1            45              0                1                         0  105.0 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.6227397648 0.0000000000 0.0000000000 0.0000000000 -0.4273626541 0.0000000000  0.0000000000 0.6554054830 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000\n",
      "4        1            47              0                1                         0  105.0 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 -0.3041077472 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 -0.4127506457 0.0000000000  0.0000000000 0.7845185065 0.0000000000 0.0000000000 -0.3488353415 0.0000000000 0.0000000000 0.0000000000  0.0000000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# -----------------------------\n",
    "# 1ï¸âƒ£ Create Master Lookup (Sets 1-17)\n",
    "# -----------------------------\n",
    "lookup_list = []\n",
    "\n",
    "for i in range(1, 18):\n",
    "    filename = f\"C:/Users/migue/Downloads/set_{i}.csv\"\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    # Remove unnamed columns if any exist\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "    if 'image_id' not in df.columns:\n",
    "        raise ValueError(f\"Set {i} missing image_id\")\n",
    "        \n",
    "    df['image_id'] = df['image_id'].astype(str).str.strip()\n",
    "    \n",
    "    # Ensure unique IDs and create time sequence\n",
    "    df = df.drop_duplicates(subset='image_id').reset_index(drop=True)\n",
    "    df['time_dataset'] = range(1, len(df) + 1)\n",
    "    df['set_id'] = i\n",
    "    \n",
    "    lookup_list.append(df[['set_id', 'image_id', 'time_dataset']])\n",
    "\n",
    "lookup_df = pd.concat(lookup_list, ignore_index=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2ï¸âƒ£ Load & Prep Target File (32 Embeddings)\n",
    "# -----------------------------\n",
    "target_path = r\"C:\\Users\\migue\\Downloads\\kosala_32_images.csv\"\n",
    "other_file = pd.read_csv(target_path, sep=';', engine='python')\n",
    "\n",
    "other_file.columns = other_file.columns.str.strip()\n",
    "other_file['image_id'] = other_file['image_id'].astype(str).str.strip()\n",
    "if 'set_id' in other_file.columns:\n",
    "    other_file['set_id'] = other_file['set_id'].astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 3ï¸âƒ£ Merge Data\n",
    "# -----------------------------\n",
    "merged_df = other_file.merge(\n",
    "    lookup_df, \n",
    "    on=['set_id', 'image_id'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4ï¸âƒ£ Post-Processing\n",
    "# -----------------------------\n",
    "\n",
    "# A. One-Hot Encode 'type'\n",
    "merged_df = pd.get_dummies(merged_df, columns=['type'], prefix='type', dtype=int)\n",
    "\n",
    "# B. Select Exact Columns (Dynamic for z0-z31)\n",
    "z_cols = [f'z{i}' for i in range(32)]\n",
    "\n",
    "base_columns = [\n",
    "    'set_id', 'time_dataset', \n",
    "    'type_adhesion', 'type_flank_wear', 'type_flank_wear+adhesion', 'wear'\n",
    "]\n",
    "\n",
    "# Ensure all columns exist before reindexing\n",
    "desired_columns = base_columns + z_cols\n",
    "final_df = merged_df.reindex(columns=desired_columns, fill_value=0)\n",
    "\n",
    "# C. Sorting\n",
    "final_df = final_df.sort_values(by=['set_id', 'time_dataset'], ascending=[True, True])\n",
    "\n",
    "# D. Clean Numeric Columns\n",
    "for col in z_cols:\n",
    "    # âš ï¸ CRITICAL: I commented this out. \n",
    "    # Only uncomment if your CSV uses dots as thousands separators (e.g. 1.000 = one thousand).\n",
    "    # If your CSV uses dots as decimals (0.5), this line destroys your data.\n",
    "    # final_df[col] = final_df[col].astype(str).str.replace('.', '', regex=False)\n",
    "    \n",
    "    # Convert to numeric safely\n",
    "    final_df[col] = pd.to_numeric(final_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "# E. Apply Normalization (L2)\n",
    "# axis=1 normalizes each ROW (embedding vector) independently.\n",
    "# This ensures the vector length is 1.\n",
    "transformer = Normalizer(norm='l2')\n",
    "final_df[z_cols] = transformer.fit_transform(final_df[z_cols])\n",
    "\n",
    "# -----------------------------\n",
    "# 5ï¸âƒ£ Save & Verify\n",
    "# -----------------------------\n",
    "final_df.to_csv(\n",
    "    \"C:/Users/migue/Downloads/images_with_time_processed_32.csv\",\n",
    "    index=False,\n",
    "    sep=';'\n",
    ")\n",
    "\n",
    "print(f\"Processing complete. Shape: {final_df.shape}\")\n",
    "print(\"First 5 rows (z-values should now be typically between -1 and 1):\")\n",
    "print(final_df.iloc[:10].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c2487610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sensor data. Rows: 1390\n",
      "â„¹ï¸ Adjusting 'sample_index' by +1 to match 'time_dataset'...\n",
      "Merge success. Rows with missing sensor data: 221\n",
      "\n",
      "--- Generating Persistent Targets ---\n",
      "Generated target_adhesion: Non-zero count = 918\n",
      "Generated target_flank: Non-zero count = 994\n",
      "Generated target_flank_adhesion: Non-zero count = 326\n",
      "Saved to: C:/Users/migue/Downloads/images_with_time_and_sensors_processed.csv\n",
      "   set_id  time_dataset     wear_level  target_adhesion   target_flank  target_flank_adhesion\n",
      "0       1             6  45.0000000000    45.0000000000   0.0000000000           0.0000000000\n",
      "1       1             8  60.0000000000    60.0000000000   0.0000000000           0.0000000000\n",
      "2       1             9  60.0000000000    60.0000000000   0.0000000000           0.0000000000\n",
      "3       1            11  75.0000000000    75.0000000000   0.0000000000           0.0000000000\n",
      "4       1            27 105.0000000000    75.0000000000 105.0000000000           0.0000000000\n",
      "5       1            28 105.0000000000    75.0000000000 105.0000000000           0.0000000000\n",
      "6       1            35 120.0000000000    75.0000000000 120.0000000000           0.0000000000\n",
      "7       1            39 120.0000000000   120.0000000000 120.0000000000           0.0000000000\n",
      "8       1            45 120.0000000000   120.0000000000 120.0000000000           0.0000000000\n",
      "9       1            47 120.0000000000   120.0000000000 120.0000000000           0.0000000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler\n",
    "\n",
    "# [Assuming final_df exists from Steps 1-5]\n",
    "\n",
    "# -----------------------------\n",
    "# 6ï¸âƒ£ Load & Process New Features File\n",
    "# -----------------------------\n",
    "features_path = r\"C:/Users/migue/Downloads/sensor_embeddings_jesse.csv\"\n",
    "features_df = pd.read_csv(features_path, sep=';', engine='python')\n",
    "\n",
    "features_df.columns = features_df.columns.str.strip()\n",
    "print(f\"Loaded sensor data. Rows: {len(features_df)}\")\n",
    "\n",
    "# A. Handle 'set' and 'sample_index' types\n",
    "if 'set' in features_df.columns:\n",
    "    features_df = features_df.dropna(subset=['set'])\n",
    "    features_df['set'] = features_df['set'].astype(int)\n",
    "\n",
    "if 'sample_index' in features_df.columns:\n",
    "    features_df = features_df.dropna(subset=['sample_index'])\n",
    "    features_df['sample_index'] = features_df['sample_index'].astype(int)\n",
    "\n",
    "# ðŸš¨ Align indices: Sensor(0) matches Image(1)\n",
    "print(\"â„¹ï¸ Adjusting 'sample_index' by +1 to match 'time_dataset'...\")\n",
    "features_df['sample_index'] = features_df['sample_index'] + 1\n",
    "\n",
    "# B. Normalize 'n', 'Vf', 'Vc' (Standard Scaler)\n",
    "process_params = ['n', 'Vf', 'Vc']\n",
    "existing_params = [col for col in process_params if col in features_df.columns]\n",
    "\n",
    "if existing_params:\n",
    "    for col in existing_params:\n",
    "        features_df[col] = pd.to_numeric(features_df[col], errors='coerce')\n",
    "        if features_df[col].std() == 0:\n",
    "            features_df[col] = 0.0\n",
    "        else:\n",
    "            scaler = StandardScaler()\n",
    "            features_df[[col]] = scaler.fit_transform(features_df[[col]])\n",
    "\n",
    "# C. Handle 'sample_index_scaled'\n",
    "if 'sample_index_scaled' in features_df.columns:\n",
    "    features_df['sample_index_scaled'] = pd.to_numeric(features_df['sample_index_scaled'], errors='coerce').fillna(0)\n",
    "    features_df['sample_index_scaled'] = features_df['sample_index_scaled'] * 10\n",
    "elif 'sample_index' in features_df.columns:\n",
    "    mm_scaler = MinMaxScaler()\n",
    "    features_df['sample_index_scaled'] = mm_scaler.fit_transform(features_df[['sample_index']]) * 10\n",
    "\n",
    "# D. Normalize Sensor Embeddings\n",
    "sens_cols = [f'sens_emb_{i}' for i in range(32)]\n",
    "existing_sens_cols = [c for c in sens_cols if c in features_df.columns]\n",
    "\n",
    "for col in existing_sens_cols:\n",
    "    features_df[col] = pd.to_numeric(features_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "if existing_sens_cols:\n",
    "    sens_transformer = Normalizer(norm='l2')\n",
    "    features_df[existing_sens_cols] = sens_transformer.fit_transform(features_df[existing_sens_cols])\n",
    "\n",
    "# -----------------------------\n",
    "# 7ï¸âƒ£ Prepare for Merge\n",
    "# -----------------------------\n",
    "features_df = features_df.rename(columns={'set': 'set_id', 'sample_index': 'time_dataset'})\n",
    "\n",
    "cols_to_keep = ['set_id', 'time_dataset', 'n', 'Vf', 'material_CK45', 'wear_level', \n",
    "                'Ae', 'fz', 'Ap', 'Vc', 'sample_index_scaled'] + existing_sens_cols\n",
    "\n",
    "final_cols_to_keep = [c for c in cols_to_keep if c in features_df.columns]\n",
    "features_df_clean = features_df[final_cols_to_keep]\n",
    "\n",
    "# -----------------------------\n",
    "# 8ï¸âƒ£ Merge\n",
    "# -----------------------------\n",
    "final_df['set_id'] = final_df['set_id'].astype(int)\n",
    "final_df['time_dataset'] = final_df['time_dataset'].astype(int)\n",
    "\n",
    "combined_df = final_df.merge(features_df_clean, on=['set_id', 'time_dataset'], how='left')\n",
    "\n",
    "# Check for NaN explosion\n",
    "if 'n' in combined_df.columns:\n",
    "    nan_count = combined_df['n'].isna().sum()\n",
    "    if nan_count == len(combined_df):\n",
    "        print(\"âŒ CRITICAL ERROR: Merge failed (100% NaNs).\")\n",
    "    else:\n",
    "        print(f\"Merge success. Rows with missing sensor data: {nan_count}\")\n",
    "\n",
    "# -----------------------------\n",
    "# ðŸ†• 9ï¸âƒ£ Create Persistent Targets (Logic Injection)\n",
    "# -----------------------------\n",
    "# Define the 3 types and corresponding new target names\n",
    "type_map = {\n",
    "    'type_adhesion': 'target_adhesion', \n",
    "    'type_flank_wear': 'target_flank', \n",
    "    'type_flank_wear+adhesion': 'target_flank_adhesion'\n",
    "}\n",
    "\n",
    "# Ensure 'wear_level' is numeric\n",
    "wear_col = 'wear_level'\n",
    "combined_df[wear_col] = pd.to_numeric(combined_df[wear_col], errors='coerce').fillna(0)\n",
    "\n",
    "print(\"\\n--- Generating Persistent Targets ---\")\n",
    "for type_col, target_col in type_map.items():\n",
    "    if type_col in combined_df.columns:\n",
    "        # 1. Mask: Keep wear value ONLY if binary type is 1. Else NaN.\n",
    "        # This initializes the 'activation' points.\n",
    "        combined_df[target_col] = combined_df[wear_col].where(combined_df[type_col] == 1)\n",
    "        \n",
    "        # 2. Forward Fill within Set: \n",
    "        # Once activated, the value propagates forward until the set ends or a new activation overrides it.\n",
    "        combined_df[target_col] = combined_df.groupby('set_id')[target_col].ffill()\n",
    "        \n",
    "        # 3. Fill Initial NaNs: Rows before the first activation become 0.\n",
    "        combined_df[target_col] = combined_df[target_col].fillna(0)\n",
    "        \n",
    "        print(f\"Generated {target_col}: Non-zero count = {(combined_df[target_col] > 0).sum()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# ðŸ”Ÿ Save\n",
    "# -----------------------------\n",
    "output_path = \"C:/Users/migue/Downloads/images_with_time_and_sensors_processed.csv\"\n",
    "combined_df.to_csv(output_path, index=False, sep=';', float_format='%.10f')\n",
    "\n",
    "print(f\"Saved to: {output_path}\")\n",
    "print(combined_df[['set_id', 'time_dataset', 'wear_level'] + list(type_map.values())].head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57953f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1112bfdf",
   "metadata": {},
   "source": [
    "# Using both the Image and the Sensor Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4ff298a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Found missing values in X. Filling with 0.\n",
      "X Shape: (1479, 75)\n",
      "y Shape: (1479, 3)\n",
      "Training Neural Network...\n",
      "\n",
      "==================================================\n",
      "ðŸ“Š MODEL PERFORMANCE (Mean Absolute Error)\n",
      "==================================================\n",
      "Global Average MAE:      15.6634\n",
      "--------------------------------------------------\n",
      "MAE for target_adhesion          : 24.4282\n",
      "MAE for target_flank             : 15.0868\n",
      "MAE for target_flank_adhesion    : 7.4751\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ðŸ” CHECKING X FEATURES (Total: 75)\n",
      "==================================================\n",
      "Feature Names:\n",
      "['type_adhesion', 'type_flank_wear', 'type_flank_wear+adhesion', 'z0', 'z1', 'z2', 'z3', 'z4', 'z5', 'z6', 'z7', 'z8', 'z9', 'z10', 'z11', 'z12', 'z13', 'z14', 'z15', 'z16', 'z17', 'z18', 'z19', 'z20', 'z21', 'z22', 'z23', 'z24', 'z25', 'z26', 'z27', 'z28', 'z29', 'z30', 'z31', 'n', 'Vf', 'material_CK45', 'Ae', 'fz', 'Ap', 'Vc', 'sample_index_scaled', 'sens_emb_0', 'sens_emb_1', 'sens_emb_2', 'sens_emb_3', 'sens_emb_4', 'sens_emb_5', 'sens_emb_6', 'sens_emb_7', 'sens_emb_8', 'sens_emb_9', 'sens_emb_10', 'sens_emb_11', 'sens_emb_12', 'sens_emb_13', 'sens_emb_14', 'sens_emb_15', 'sens_emb_16', 'sens_emb_17', 'sens_emb_18', 'sens_emb_19', 'sens_emb_20', 'sens_emb_21', 'sens_emb_22', 'sens_emb_23', 'sens_emb_24', 'sens_emb_25', 'sens_emb_26', 'sens_emb_27', 'sens_emb_28', 'sens_emb_29', 'sens_emb_30', 'sens_emb_31']\n",
      "--------------------------------------------------\n",
      "First 5 Rows of X (All columns visible):\n",
      "   type_adhesion  type_flank_wear  type_flank_wear+adhesion           z0           z1           z2           z3           z4           z5           z6           z7           z8           z9          z10          z11          z12          z13          z14          z15          z16          z17          z18          z19          z20          z21          z22           z23          z24          z25          z26           z27          z28          z29          z30           z31            n           Vf  material_CK45           Ae           fz           Ap           Vc  sample_index_scaled   sens_emb_0   sens_emb_1   sens_emb_2    sens_emb_3   sens_emb_4    sens_emb_5   sens_emb_6    sens_emb_7   sens_emb_8   sens_emb_9  sens_emb_10  sens_emb_11   sens_emb_12  sens_emb_13  sens_emb_14  sens_emb_15  sens_emb_16  sens_emb_17  sens_emb_18  sens_emb_19  sens_emb_20  sens_emb_21  sens_emb_22  sens_emb_23  sens_emb_24  sens_emb_25  sens_emb_26   sens_emb_27  sens_emb_28  sens_emb_29  \\\n",
      "0              1                0                         0 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.1416723815 0.2019604467   1.0000000000 1.0000000000 0.0480000000 1.0000000000 0.1453474952         0.7042253521 0.0000000000 0.4556407969 0.0000000000 -0.1718885378 0.0000000000 -0.2710802729 0.4221491405 -0.0747449344 0.2468326897 0.0000000000 0.0000000000 0.0000000000 -0.3785527280 0.0000000000 0.1124298539 0.0000000000 0.0000000000 0.0000000000 0.3726338423 0.2211814686 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.2474166445 0.0000000000 0.0000000000  0.0208415723 0.0000000000 0.1168703239   \n",
      "1              1                0                         0 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.7219238094 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 -0.5031689453 0.0000000000 0.0000000000 0.0000000000  0.4750231845 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.1416723815 0.2019604467   1.0000000000 1.0000000000 0.0480000000 1.0000000000 0.1453474952         0.9859154930 0.0000000000 0.4065944276 0.0000000000 -0.1551909726 0.4530802346 -0.3126058667 0.3841004690 -0.0407282407 0.1569558017 0.0000000000 0.0000000000 0.0000000000 -0.3003010661 0.0000000000 0.1210262075 0.0000000000 0.0000000000 0.0000000000 0.3422098812 0.1396332088 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.2589903179 0.0000000000 0.0000000000 -0.0627803685 0.0000000000 0.0798320404   \n",
      "2              1                0                         0 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000 -1.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.1416723815 0.2019604467   1.0000000000 1.0000000000 0.0480000000 1.0000000000 0.1453474952         1.1267605634 0.0000000000 0.4413325112 0.0000000000 -0.1403642338 0.3940324725 -0.3485215571 0.3638182641 -0.0069338102 0.1417747800 0.0000000000 0.0000000000 0.0000000000 -0.2687119376 0.0000000000 0.1378888522 0.0000000000 0.0000000000 0.0000000000 0.3792955173 0.1124401185 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.2910913437 0.0000000000 0.0000000000 -0.1229463372 0.0000000000 0.0321839186   \n",
      "3              1                0                         0 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.1416723815 0.2019604467   1.0000000000 1.0000000000 0.0480000000 1.0000000000 0.1453474952         1.4084507042 0.0000000000 0.4273546351 0.0000000000 -0.1680396566 0.3277833058 -0.3787747203 0.4339523122 -0.0082671342 0.0833299980 0.0000000000 0.0000000000 0.0000000000 -0.2840967705 0.0000000000 0.1708636239 0.0000000000 0.0000000000 0.0000000000 0.3244819809 0.1079402895 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.2764168008 0.0000000000 0.0000000000 -0.1652277457 0.0000000000 0.0478894528   \n",
      "4              0                1                         0 0.0000000000 0.0000000000 0.0000000000 0.8560216173 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.4980404168 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000  0.0000000000 0.0000000000 0.0000000000 0.0000000000 -0.1385017473 0.1416723815 0.2019604467   1.0000000000 1.0000000000 0.0480000000 1.0000000000 0.1453474952         3.6619718310 0.0000000000 0.4031970512 0.4481751457 -0.1424223492 0.2898510684 -0.3495139211 0.3437383314  0.0815521855 0.0124096691 0.0000000000 0.0000000000 0.0000000000 -0.2900605811 0.0000000000 0.2343103337 0.0000000000 0.0000000000 0.0000000000 0.1392890865 0.1465704527 0.0000000000 0.0000000000 0.0000000000 0.0000000000 0.2867674767 0.0000000000 0.0000000000 -0.1006184329 0.0000000000 0.0491172401   \n",
      "\n",
      "   sens_emb_30  sens_emb_31  \n",
      "0 0.1600321824 0.0000000000  \n",
      "1 0.1222370645 0.0000000000  \n",
      "2 0.0875765301 0.0000000000  \n",
      "3 0.1002336849 0.0000000000  \n",
      "4 0.1022724328 0.0000000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Force Pandas to display ALL columns (no \"...\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# 1. Define Target Columns (y)\n",
    "y_cols = ['target_adhesion', 'target_flank', 'target_flank_adhesion']\n",
    "\n",
    "# 2. Define Features (X)\n",
    "# UPDATED: We now drop 'set_id' and 'time_dataset' alongside the leakage columns\n",
    "drop_cols = y_cols + ['wear_level', 'wear', 'set_id', 'time_dataset']\n",
    "\n",
    "# Safe drop (ignores if any column doesn't exist)\n",
    "X = combined_df.drop(columns=[c for c in drop_cols if c in combined_df.columns])\n",
    "y = combined_df[y_cols]\n",
    "\n",
    "# -----------------------------\n",
    "# ðŸš¨ CRITICAL FIX: Handle NaNs from Merge\n",
    "# -----------------------------\n",
    "# Fill missing sensor data with 0 (Mean/Zero Vector)\n",
    "if X.isna().sum().sum() > 0:\n",
    "    print(f\"âš ï¸ Found missing values in X. Filling with 0.\")\n",
    "    X = X.fillna(0)\n",
    "if y.isna().sum().sum() > 0:\n",
    "    y = y.fillna(0)\n",
    "\n",
    "print(f\"X Shape: {X.shape}\")\n",
    "print(f\"y Shape: {y.shape}\")\n",
    "\n",
    "# 3. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\n",
    "\n",
    "# 4. Train Neural Network\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    \n",
    "    # 1. LEARNING RATE\n",
    "    learning_rate_init=0.0003,      # Default is 0.001. Lower = more precise but slower.\n",
    "    \n",
    "    # 2. OVERFITTING PROTECTION (L2 Regularization)\n",
    "    alpha=0.01,                     # Default is 0.0001. Increase (e.g., 0.01, 0.1) to reduce overfitting.\n",
    "    \n",
    "    # 3. EARLY STOPPING\n",
    "    early_stopping=True,            # Stop if validation score doesn't improve\n",
    "    validation_fraction=0.1,        # Use 10% of train data to check for early stopping\n",
    "    n_iter_no_change=10,            # Stop if no improvement for 10 epochs\n",
    "    \n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Training Neural Network...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluate with MAE\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate MAE for each individual target column\n",
    "mae_per_col = mean_absolute_error(y_test, y_pred, multioutput='raw_values')\n",
    "total_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸ“Š MODEL PERFORMANCE (Mean Absolute Error)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Global Average MAE:      {total_mae:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "for i, col in enumerate(y_cols):\n",
    "    print(f\"MAE for {col:<25}: {mae_per_col[i]:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 6. Print X Columns and Head (Checking the features)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"ðŸ” CHECKING X FEATURES (Total: {len(X.columns)})\")\n",
    "print(\"=\"*50)\n",
    "print(\"Feature Names:\")\n",
    "print(X.columns.tolist())\n",
    "print(\"-\" * 50)\n",
    "print(\"First 5 Rows of X (All columns visible):\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63921017",
   "metadata": {},
   "source": [
    "# Just the image embeddings for a NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f08ed6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Found NaNs in X (filling with 0)...\n",
      "X Shape: (1479, 43)\n",
      "y Shape: (1479, 3)\n",
      "\n",
      "Training Image-Only Neural Network...\n",
      "\n",
      "==================================================\n",
      "ðŸ“Š MODEL PERFORMANCE (Image Embeddings Only)\n",
      "==================================================\n",
      "Train MAE:           12.5340\n",
      "Test MAE (Global):   15.6206\n",
      "--------------------------------------------------\n",
      "MAE for target_adhesion          : 24.6791\n",
      "MAE for target_flank             : 14.2056\n",
      "MAE for target_flank_adhesion    : 7.9770\n",
      "==================================================\n",
      "\n",
      "Features used in X (Should be z0-z31 + type columns):\n",
      "['type_adhesion', 'type_flank_wear', 'type_flank_wear+adhesion', 'z0', 'z1', 'z2', 'z3', 'z4', 'z5', 'z6', 'z7', 'z8', 'z9', 'z10', 'z11', 'z12', 'z13', 'z14', 'z15', 'z16', 'z17', 'z18', 'z19', 'z20', 'z21', 'z22', 'z23', 'z24', 'z25', 'z26', 'z27', 'z28', 'z29', 'z30', 'z31', 'n', 'Vf', 'material_CK45', 'Ae', 'fz', 'Ap', 'Vc', 'sample_index_scaled']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 1. Define Target Columns (y)\n",
    "y_cols = ['target_adhesion', 'target_flank', 'target_flank_adhesion']\n",
    "\n",
    "# 2. Define Features to DROP (As requested)\n",
    "# We strictly remove all sensors, physical params, and the answers (targets/wear_level)\n",
    "cols_to_drop = [\n",
    "    # Physical Params\n",
    "    \n",
    "    # Identifiers (Prevent memorization)\n",
    "    'set_id', 'time_dataset', 'wear', 'wear_level',\n",
    "    # Targets (Answers)\n",
    "    'target_adhesion', 'target_flank', 'target_flank_adhesion'\n",
    "]\n",
    "\n",
    "# Add Sensor Embeddings 0-31 to the drop list\n",
    "cols_to_drop += [f'sens_emb_{i}' for i in range(32)]\n",
    "\n",
    "# 3. Create X and y\n",
    "# Note: X will now contain mostly 'z0'-'z31' and the 'type_...' binary columns\n",
    "X = combined_df.drop(columns=[c for c in cols_to_drop if c in combined_df.columns])\n",
    "y = combined_df[y_cols]\n",
    "\n",
    "# 4. Handle NaNs (Safety)\n",
    "if X.isna().sum().sum() > 0:\n",
    "    print(\"âš ï¸ Found NaNs in X (filling with 0)...\")\n",
    "    X = X.fillna(0)\n",
    "if y.isna().sum().sum() > 0:\n",
    "    y = y.fillna(0)\n",
    "\n",
    "print(f\"X Shape: {X.shape}\")\n",
    "print(f\"y Shape: {y.shape}\")\n",
    "\n",
    "# 5. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\n",
    "\n",
    "# 6. Train Neural Network (With Regularization to fight overfitting)\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(128, 64), \n",
    "    activation='relu',             \n",
    "    solver='adam',                 \n",
    "    learning_rate_init=0.0003,      \n",
    "    alpha=0.05,                    # L2 Regularization (Higher = less overfitting)\n",
    "    early_stopping=True,           # Stop if validation score stalls\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    max_iter=1000,                  \n",
    "    random_state=42,\n",
    "    verbose=False                   \n",
    ")\n",
    "\n",
    "print(\"\\nTraining Image-Only Neural Network...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate (MAE)\n",
    "y_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)\n",
    "\n",
    "# Calculate Errors\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "train_mae = mean_absolute_error(y_train, train_pred)\n",
    "mae_per_col = mean_absolute_error(y_test, y_pred, multioutput='raw_values')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸ“Š MODEL PERFORMANCE (Image Embeddings Only)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Train MAE:           {train_mae:.4f}\")\n",
    "print(f\"Test MAE (Global):   {test_mae:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "for i, col in enumerate(y_cols):\n",
    "    print(f\"MAE for {col:<25}: {mae_per_col[i]:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 8. Verify Input Features\n",
    "print(\"\\nFeatures used in X (Should be z0-z31 + type columns):\")\n",
    "print(X.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
